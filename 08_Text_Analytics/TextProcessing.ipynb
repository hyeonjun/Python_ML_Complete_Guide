{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextProcessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPc37EqSxy9Fj9Y7kg8A/p"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 전처리 - 텍스트 정규화\n",
        "\n",
        ": 텍스트를 클렌징, 정제, 토큰화, 어근화 등의 다양한 텍스트 데이터의 사전 작업을 수행.\n",
        "\n",
        "* 클렌징(Cleansing) : 분석에 불필요한 문자, 기호 등을 사전 제거. ex) html, xml 태그나 특정 기호\n",
        "* 토큰화(Tokenization) : 문서에서 문장을 분리하는 문장 토큰화와 문장에서 단어를 토큰으로 분리하는 단어 토큰화. NLTK\n",
        "    * 문장 토큰화 - 문장의 마침표, 개행문자 등 문장의 마지막을 뜻하는 기호 또는 정규표현식에 따라 분리\n",
        "        * sent_tokenize()\n",
        "    * 단어 토큰화 - 공백, 콤마, 마침표, 개행문자 또는 정규 표현식 등으로 단어 분리\n",
        "        * BoW 같이 단어의 순서가 중요하지 않은 경우 토큰화를 사용하지 않고 단어 토큰화만 사용해도 충분\n",
        "        * word_tokenize()\n"
      ],
      "metadata": {
        "id": "BAORVPJIDLWq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c79nVf9bDH6w",
        "outputId": "4e4db9a8-2609-488a-be48-fa368752c601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<class 'list'> 3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Matrix is everywhere its all around us, here even in this room.',\n",
              " 'You can see it out your window or on your television.',\n",
              " 'You feel it when you go to work, or go to church or pay your taxes.']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "# 문장 토큰화\n",
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "               You can see it out your window or on your television. \\\n",
        "               You feel it when you go to work, or go to church or pay your taxes.'\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(type(sentences), len(sentences))\n",
        "sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sent_tokenize()가 반환하는 것은 각각의 문장으로 구성된 list 객체이다. \n",
        "\n",
        "반환된 list 객체가 3개의 문장으로 된 문자열을 가지고 있는 것을 알 수 있다."
      ],
      "metadata": {
        "id": "0gqd6pQlNPs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentences = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
        "words = word_tokenize(sentences)\n",
        "print(type(words), len(words))\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2_6X0utNN4_",
        "outputId": "1fba601f-16c6-4262-82c5-6184a569fb3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sent_tokenize와 word_tokenize를 조합해 문서에 대해서 모든 단어를 토근화.\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
        "def tokenize_text(text):\n",
        "    # 문장별로 분리 토큰\n",
        "    sentences = sent_tokenize(text)\n",
        "    # 분리된 문장별 단어 토큰화\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "    return word_tokens\n",
        "\n",
        "#여러 문장들에 대해 문장별 단어 토큰화 수행. \n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens),len(word_tokens))\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsvcq4bEN5Ff",
        "outputId": "a73da59e-3d39-4208-a932-b0c42df3bdf4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3개 문장을 문장별로 먼저 토큰화했으므로 word_tokens 변수는 3개의 리스트 객체를 내포하는 리스트이다. 그리고 내포된 개별 리스트 객체는 각각 문장별로 토큰화된 단어를 요소로 갖고있다.\n",
        "\n",
        "문장을 단어별로 하나씩 토큰화할 경우 문맥적인 의미는 무시된다. 이러한 문제를 조금 해결하기위해 도입된 것이 n-gram이다. n-gram은 연속된 n개의 단어를 하나의 토큰화 단위로 분리하는 것이다.\n",
        "\n",
        "ex) \"Agent Smith Knocks the door\" == 2-gram(bigram) ==> (Agent, Smith), (Smith, knocks), (knocks, the), (the, door)"
      ],
      "metadata": {
        "id": "vOuLR8IlO9gO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 필터링 / 스톱 워드 제거 / 철자 수정\n",
        "    * 문장을 구성하는 필수 문법 요소지만 문맥적으로 큰 의미가 없는 단어\n",
        "    * 문법적인 특성으로 인해 빈번하게 등장하여 중요 단어로 인식될 수 있어 제거 필요"
      ],
      "metadata": {
        "id": "35FJdz5cQT-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # 가장 다양한 언어의 스톱 워드를 제공\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q-hVtGwQr7v",
        "outputId": "b7d0ed1e-2c06-4367-91bd-d0d79a307730"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('영어 stop words 개수 :', len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yJejgCMQ9zu",
        "outputId": "b0055eaa-b4b4-4f35-fe6b-99afe887bdbb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 stop words 개수 : 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "영어의 경우 스톱 워크의 개수가 179개이다.\n",
        "\n",
        "3개의 문장별로 단어를 토큰화해 생성된 word_tokens 리스트(3개의 문장별로 단어 토큰화 값을 가지는 내포된 리스트로 구성)에 대해서 stopwords를 필터링으로 제거해 분석을 위한 의미있는 단어만 추출."
      ],
      "metadata": {
        "id": "T4ALOL1IRPRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "# 위의 3개의 문장별로 얻은 word_tokens list에 대해 스톱 워드를 제거하는 반복문\n",
        "for sentence in word_tokens:\n",
        "    filtered_words = []\n",
        "    # 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반복문\n",
        "    for word in sentence:\n",
        "        # 소문자로 모두 변환한다.\n",
        "        word = word.lower()\n",
        "        # 토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens 에 추가\n",
        "        if word not in stopwords:\n",
        "            filtered_words.append(word)\n",
        "    all_tokens.append(filtered_words)\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga5VqiMORiIH",
        "outputId": "e6418c69-0086-44e6-bf42-e3ac592c6b7e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "is, this와 같은 스톱 워드가 필터링을 통해 제거되었다."
      ],
      "metadata": {
        "id": "MV53XFB_SNQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Stemming과 Lemmatization\n",
        "    * 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 것(시제, 3인칭 단수 여부 등)\n",
        "    * Stemming : 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용해 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 경향이 있음.\n",
        "        * NLTK.LancasterStemmer, Porter, Lancaster, Snowball Stemmer\n",
        "    * Lemmatization : 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾아준다.\n",
        "        * NLTK.WordNetLemmatizer\n",
        "    * Lemmatization이 Stemming보다 더 정교하지만 변환에 더 오랜 시간이 걸린다."
      ],
      "metadata": {
        "id": "66MZieLAMQK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "# Stemmer 객체를 생성한 뒤 이 객체의 stem('단어') 메서드를 호출하면 원하는 '단어'의 Stemming이 가능.\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXYEMIlMXJJc",
        "outputId": "b5948ccf-936a-4c6b-a732-9d8507d1ffc1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "work의 경우 진행형, 3인칭 단수, 과거형 모두 기본형인 work에 ing, s, ed가 붙는 단순한 변화이므로 원형 단어인 work로 제대로 인식된다. \n",
        "\n",
        "하지만 amuse의 경우 각 변화가 amuse가 아닌 amus에 ing, s, ed가 붙으므로 amuse가 아닌 amus를 원형 데이터 생각한다.\n",
        "\n",
        "형용사인 happy, fancy의 경우 비교형, 최상급형으로 변형된 단어의 정확한 원형을 찾지 못하고 원형 단어에서 철자가 다른 어근 단어로 인식하는 경우가 발생한다."
      ],
      "metadata": {
        "id": "eHlljG9ZXoZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "# 보다 정확한 원형 단어 추출을 위해 단어의 품사를 입력해줘야함.\n",
        "# v : 동사, a : 형용사\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
        "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
        "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYgHirNwYYUU",
        "outputId": "39189aff-c96b-4e37-e100-7ebd70b820d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ]
        }
      ]
    }
  ]
}